---
title: "Description"
bg: white
color: black
style: center
---
# How2 Challenge
{: .text-purple}

### The How2 Challenge - New Tasks for Vision and Language

*Research at the intersection of vision and language has been attracting a lot of attention in recent years. Research topics include the study of multi-modal representations, translation between modalities, bootstrapping of labels from one modality into another, visually-grounded question answering, segmentation and storytelling, and grounding the meaning of language in visual data. An ever-increasing number of tasks and datasets are appearing around this recently-established field.

At NeurIPS 2018, we released the How2 data-set, containing 85,000 (2000h) of how-to videos, with audio, sub-titles, translations, and textual summaries, making it an ideal resource to bring together researchers working on these separate tasks. We hope that a common dataset will facilitate comparisons of tools and algorithms, and foster collaboration. Hence, we present this workshop at ICML 2019. The How2 data-set has already been released, and several groups aside from the proposers have actively started to explore its tasks. We encourage you to take a look, download it, and build whatever models you find interesting with it!

The workshop will be held on June 14/15th. Stay tuned for more information and updates!*



