---
title: "Call"
bg: white
color: black
style: left
---

### Call For Papers

We seek submissions in the following two categories:


- Papers that describe work on the How2 data, either on the shared challenge tasks, e.g. multi-modal speech recognition (<a href="https://arxiv.org/pdf/1804.09713.pdf">Palaskar et al. ICASSP 2018</a>, <a href="https://arxiv.org/pdf/1811.03865.pdf">Caglayan et al. ICASSP 2019</a>), machine translation (<a href="https://www.statmt.org/wmt18/multimodal-task.html">Shared task on Multimodal MT</a>), or video summarization (<a href="https://nips2018vigil.github.io/static/papers/accepted/8.pdf">Libovicky et al. ViGIL, NeurIPS 2018</a>), or creating "un-shared", novel tasks that create language, speech and/or vision.


- Examples of novel tasks could be spoken language translation, cross-modal multimodal learning, unsupervised representation learning (<a href="https://arxiv.org/abs/1811.08890">Holzenberger et al. ICASSP 2019</a>), reasoning in vision and language, visual synthesis from language, vision and language interaction for humans, learning from in-the-wild videos (<a href="https://arxiv.org/pdf/1811.00347.pdf">How2 data</a> or others), lip reading, audio-visual scene understanding, sound localization, multimodal fusion, visual question answering, and many more.


- Papers that describe other related and relevant work to further vision and language ideas by proposing new tasks, or analyzing the utility of existing tasks and data sets in interesting ways


We encourage both the publication of novel work that is relevant to the topics of discussion, and late-breaking results on the How2 tasks in a single format. We aim to stimulate discussion around new tasks that go beyond image captioning and visual question answering, and which could form the basis for future research in this area. 

* * *
